% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizer.R
\name{Tokenizers}
\alias{Tokenizers}
\alias{tokenizer_delim}
\alias{tokenizer_csv}
\alias{tokenizer_tsv}
\alias{tokenizer_line}
\alias{tokenizer_log}
\alias{tokenizer_fwf}
\alias{tokenizer_ws}
\title{Tokenizers.}
\usage{
tokenizer_delim(delim, quote = "\\"", na = "NA", quoted_na = TRUE,
  comment = "", trim_ws = TRUE, escape_double = TRUE,
  escape_backslash = FALSE)

tokenizer_csv(na = "NA", quoted_na = TRUE, quote = "\\"", comment = "",
  trim_ws = TRUE)

tokenizer_tsv(na = "NA", quoted_na = TRUE, quote = "\\"", comment = "",
  trim_ws = TRUE)

tokenizer_line(na = character())

tokenizer_log()

tokenizer_fwf(begin, end, na = "NA", comment = "", trim_ws = TRUE)

tokenizer_ws(na = "NA", comment = "")
}
\arguments{
\item{delim}{Single character used to separate fields within a record.}

\item{quote}{Single character used to quote strings.}

\item{na}{Character vector of strings to use for missing values. Set this
option to \code{character()} to indicate no missing values.}

\item{quoted_na}{Should missing values inside quotes be treated as missing
values (the default) or strings.}

\item{comment}{This argument is deprecated and will be ignored. Comments are
now handled by \code{\link[=datasource]{datasource()}}.}

\item{trim_ws}{Should leading and trailing whitespace be trimmed from
each field before parsing it?}

\item{escape_double}{Does the file escape quotes by doubling them?
i.e. If this option is \code{TRUE}, the value \code{""""} represents
a single quote, \code{\"}.}

\item{escape_backslash}{Does the file use backslashes to escape special
characters? This is more general than \code{escape_double} as backslashes
can be used to escape the delimiter character, the quote character, or
to add special characters like \code{\\n}.}

\item{begin, end}{Begin and end offsets for each file. These are C++
offsets so the first column is column zero, and the ranges are
[begin, end) (i.e inclusive-exclusive).}
}
\description{
Explicitly create tokenizer objects. Usually you will not call these
function, but will instead use one of the use friendly wrappers like
\code{\link[=read_csv]{read_csv()}}.
}
\examples{
tokenizer_csv()
}
\keyword{internal}
